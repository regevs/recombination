# ------------------------------------------------------------------------------------------------------------------------
# 1. Analyze reads
#
import pysam
rule read_refinement:
    input:
        bam_filename_1 = Path("/lustre/scratch126/casm/team154pc/sl17/03.sperm/02.results/01.read_alignment/01.ccs/04.hifiasm/02.hifiasm_0.19.5-r592/02.chromosome_length_scaffolds/") \
            / "{focal_sample_id}" / "{focal_sample_id}.hap1.minimap2.sorted.primary_alignments.bam",
        bam_filename_2 = Path("/lustre/scratch126/casm/team154pc/sl17/03.sperm/02.results/01.read_alignment/01.ccs/04.hifiasm/02.hifiasm_0.19.5-r592/02.chromosome_length_scaffolds/") \
            / "{focal_sample_id}" / "{focal_sample_id}.hap2.minimap2.sorted.primary_alignments.bam",
    output:
        parquet = Path("/lustre/scratch126/casm/team154pc/sl17/03.sperm/02.results/01.read_alignment/01.ccs/04.hifiasm/02.hifiasm_0.19.5-r592/02.chromosome_length_scaffolds/") \
            / "{focal_sample_id}" / "reads" / "{chrom}.read_refinement.parquet",
    threads: 8,
    resources:
        mem_mb=64000,
    run:
        print("Running...")
        cdf = diagnostics.run_all_refine_cigars(
            input.bam_filename_1,
            input.bam_filename_2,
            wildcards.chrom,
            threads,
        )

        print("Writing...")        
        cdf.write_parquet(
            output.parquet,
        )

rule read_refinement_final:
    input:
        csv_gz = [str(Path("/lustre/scratch126/casm/team154pc/sl17/03.sperm/02.results/01.read_alignment/01.ccs/04.hifiasm/02.hifiasm_0.19.5-r592/02.chromosome_length_scaffolds/") \
                / f"{focal_sample_id}" / "reads" / f"{chrom + '_RagTag'}.read_refinement.parquet") \
                for focal_sample_id in sample_ids \
                for chrom in aut_chrom_names] 


# ------------------------------------------------------------------------------------------------------------------------
# 2. Call and annotate high confidence SNPs
#
rule create_high_conf_snps:
    input:
        refinement_parquet = Path("/lustre/scratch126/casm/team154pc/sl17/03.sperm/02.results/01.read_alignment/01.ccs/04.hifiasm/02.hifiasm_0.19.5-r592/02.chromosome_length_scaffolds/") \
            / "{focal_sample_id}" / "reads" / "{chrom}.read_refinement.parquet",
        trf_dat1 = hap_scaffolds_path \
            / "{focal_sample_id}" / "haplotype_1" / "ragtag.scaffold.expanded.fasta.{chrom}.trf.dat",
        trf_dat2 = hap_scaffolds_path \
            / "{focal_sample_id}" / "haplotype_2" / "ragtag.scaffold.expanded.fasta.{chrom}.trf.dat",
        sdust_dat1 = hap_scaffolds_path \
            / "{focal_sample_id}" / "haplotype_1" / "ragtag.scaffold.expanded.fasta.{chrom}.sdust.tsv",
        sdust_dat2 = hap_scaffolds_path \
            / "{focal_sample_id}" / "haplotype_2" / "ragtag.scaffold.expanded.fasta.{chrom}.sdust.tsv",
    output:
        parquet = Path("/lustre/scratch126/casm/team154pc/sl17/03.sperm/02.results/01.read_alignment/01.ccs/04.hifiasm/02.hifiasm_0.19.5-r592/02.chromosome_length_scaffolds/") \
            / "{focal_sample_id}" / "reads" / "{chrom}.high_confidence_snps.parquet",
    resources:
        mem_mb = 64000,
    run:
        # Create SNPs dataframe
        events_df = pl.read_parquet(input.refinement_parquet)
        events_df = diagnostics.filter_read_refinements(events_df)

        print("Done filter_read_refinements")

        annotated_events_df = diagnostics.extract_high_conf_events(
            events_df,
            high_confidence_slack = 10,
        )

        print("Done extract_high_conf_events")

        # Add TRF annotations
        annotated_events_df = diagnostics.add_tandem_repeat_finder_annotation(
            annotated_events_df,
            input.trf_dat1,
            input.trf_dat2,
            pl.col("is_flanked_event"),
        )

        print("Done add_tandem_repeat_finder_annotation")

        # Add sdust annotations
        annotated_events_df = diagnostics.add_sdust_annotation(
            annotated_events_df,
            input.sdust_dat1,
            input.sdust_dat2,
            pl.col("is_flanked_event"),
        )

        annotated_events_df = annotated_events_df.collect(streaming=True).lazy()
        print("Done add_sdust_annotation")

        # Add high-quality status annotation
        annotated_events_df = diagnostics.add_high_confidence_annotation(            
            annotated_events_df, 
            output_column_prefix = "is_high_conf",
            base_qual_min = 60,
            read_trimming = 1500,
        )

        # Add mid-quality status annotation
        annotated_events_df = diagnostics.add_high_confidence_annotation(            
            annotated_events_df, 
            output_column_prefix = "is_mid_conf",
            base_qual_min = 30,
            read_trimming = 500,
        )

        annotated_events_df = annotated_events_df.collect(streaming=True)
        print("Done add_high_confidence_annotation")

        # Write
        annotated_events_df.write_parquet(output.parquet)

rule create_high_conf_snps_all:
    input:
        parquet = [str(Path("/lustre/scratch126/casm/team154pc/sl17/03.sperm/02.results/01.read_alignment/01.ccs/04.hifiasm/02.hifiasm_0.19.5-r592/02.chromosome_length_scaffolds/") \
                    / f"{focal_sample_id}" / "reads" / f"{chrom}_RagTag.high_confidence_snps.parquet") \
            for focal_sample_id in sample_ids[:1] \
            for chrom in aut_chrom_names[:1]] 

# ------------------------------------------------------------------------------------------------------------------------
# 3. Phase and haplotag according to high confidence SNPs
#

rule phase_and_haplotag:
    input:
        parquet = Path("/lustre/scratch126/casm/team154pc/sl17/03.sperm/02.results/01.read_alignment/01.ccs/04.hifiasm/02.hifiasm_0.19.5-r592/02.chromosome_length_scaffolds/") \
            / "{focal_sample_id}" / "reads" / "{chrom}.high_confidence_snps.parquet",
        bam1 = Path("/lustre/scratch126/casm/team154pc/sl17/03.sperm/02.results/01.read_alignment/01.ccs/04.hifiasm/02.hifiasm_0.19.5-r592/02.chromosome_length_scaffolds/") \
            / "{focal_sample_id}" / "{focal_sample_id}.hap1.minimap2.sorted.primary_alignments.bam",
        bam2 = Path("/lustre/scratch126/casm/team154pc/sl17/03.sperm/02.results/01.read_alignment/01.ccs/04.hifiasm/02.hifiasm_0.19.5-r592/02.chromosome_length_scaffolds/") \
            / "{focal_sample_id}" / "{focal_sample_id}.hap2.minimap2.sorted.primary_alignments.bam",
    output:
        bam1 = Path("/lustre/scratch126/casm/team154pc/sl17/03.sperm/02.results/01.read_alignment/01.ccs/04.hifiasm/02.hifiasm_0.19.5-r592/02.chromosome_length_scaffolds/") \
            / "{focal_sample_id}" / "{focal_sample_id}.{chrom}.hap1.minimap2.sorted.primary_alignments.hifiasm_haplotagged_{certainty}.bam",
        bam_bai1 = Path("/lustre/scratch126/casm/team154pc/sl17/03.sperm/02.results/01.read_alignment/01.ccs/04.hifiasm/02.hifiasm_0.19.5-r592/02.chromosome_length_scaffolds/") \
            / "{focal_sample_id}" / "{focal_sample_id}.{chrom}.hap1.minimap2.sorted.primary_alignments.hifiasm_haplotagged_{certainty}.bam.bai",
        bam2 = Path("/lustre/scratch126/casm/team154pc/sl17/03.sperm/02.results/01.read_alignment/01.ccs/04.hifiasm/02.hifiasm_0.19.5-r592/02.chromosome_length_scaffolds/") \
            / "{focal_sample_id}" / "{focal_sample_id}.{chrom}.hap2.minimap2.sorted.primary_alignments.hifiasm_haplotagged_{certainty}.bam",
        bam_bai2 = Path("/lustre/scratch126/casm/team154pc/sl17/03.sperm/02.results/01.read_alignment/01.ccs/04.hifiasm/02.hifiasm_0.19.5-r592/02.chromosome_length_scaffolds/") \
            / "{focal_sample_id}" / "{focal_sample_id}.{chrom}.hap2.minimap2.sorted.primary_alignments.hifiasm_haplotagged_{certainty}.bam.bai",
    resources:
        mem_mb=32000,
    run:
        diagnostics.phase_and_haplotag(
            input.parquet,
            input.bam1,
            input.bam2,
            output.bam1,
            output.bam2,
            certainty_threshold=float(wildcards.certainty),
        )

        shell(
            "{samtools_path} index {output.bam1}"
        )

        shell(
            "{samtools_path} index {output.bam2}"
        )

rule calculate_genome_coverage_on_haplotype:
    input:
        bam = Path("/lustre/scratch126/casm/team154pc/sl17/03.sperm/02.results/01.read_alignment/01.ccs/04.hifiasm/02.hifiasm_0.19.5-r592/02.chromosome_length_scaffolds/") \
            / "{focal_sample_id}" / "{focal_sample_id}.{chrom}.hap{haplotype}.minimap2.sorted.primary_alignments.hifiasm_haplotagged_{certainty}.bam",
        bam_bai = Path("/lustre/scratch126/casm/team154pc/sl17/03.sperm/02.results/01.read_alignment/01.ccs/04.hifiasm/02.hifiasm_0.19.5-r592/02.chromosome_length_scaffolds/") \
            / "{focal_sample_id}" / "{focal_sample_id}.{chrom}.hap{haplotype}.minimap2.sorted.primary_alignments.hifiasm_haplotagged_{certainty}.bam.bai",
    output:
        bedgraph = Path("/lustre/scratch126/casm/team154pc/sl17/03.sperm/02.results/01.read_alignment/01.ccs/04.hifiasm/02.hifiasm_0.19.5-r592/02.chromosome_length_scaffolds/") \
            / "{focal_sample_id}" / "{focal_sample_id}.{chrom}.hap{haplotype}.minimap2.sorted.primary_alignments.hifiasm_haplotagged_{certainty}.coverage.bedgraph",
    resources:
        mem_mb=4000,
    run:
        shell(
            "{samtools_path} view --with-header --tag HP:{wildcards.haplotype} {input.bam} | "
            "{bedtools_path} genomecov -ibam - -bg > {output.bedgraph}"
        )

# rule intersect_coverage_two_haplotypes:
#     input:
#         bedgraph1 = Path("/lustre/scratch126/casm/team154pc/sl17/03.sperm/02.results/01.read_alignment/01.ccs/04.hifiasm/02.hifiasm_0.19.5-r592/02.chromosome_length_scaffolds/") \
#                     / "{focal_sample_id}" / "{focal_sample_id}.{chrom}.hap1.minimap2.sorted.primary_alignments.hifiasm_haplotagged_{certainty}.coverage.bedgraph",
#         bedgraph2 = Path("/lustre/scratch126/casm/team154pc/sl17/03.sperm/02.results/01.read_alignment/01.ccs/04.hifiasm/02.hifiasm_0.19.5-r592/02.chromosome_length_scaffolds/") \
#                     / "{focal_sample_id}" / "{focal_sample_id}.{chrom}.hap2.minimap2.sorted.primary_alignments.hifiasm_haplotagged_{certainty}.coverage.bedgraph",
#     output:
#         bedgraph = Path("/lustre/scratch126/casm/team154pc/sl17/03.sperm/02.results/01.read_alignment/01.ccs/04.hifiasm/02.hifiasm_0.19.5-r592/02.chromosome_length_scaffolds/") \
#                     / "{focal_sample_id}" / "{focal_sample_id}.{chrom}.hapintersection.minimap2.sorted.primary_alignments.hifiasm_haplotagged_{certainty}.coverage.bedgraph",
#     resources:
#         mem_mb=4000,
#     run:
#         shell(
#             "{bedtools_path} intersect -a {input.bedgraph1} -b {input.bedgraph2} -wo > {output.bedgraph}"
#         )

rule phase_and_haplotag_final:
    input:
        bam = [str(Path("/lustre/scratch126/casm/team154pc/sl17/03.sperm/02.results/01.read_alignment/01.ccs/04.hifiasm/02.hifiasm_0.19.5-r592/02.chromosome_length_scaffolds/") \
            / f"{focal_sample_id}" / f"{focal_sample_id}.{chrom}_RagTag.hap{haplotype}.minimap2.sorted.primary_alignments.hifiasm_haplotagged_{certainty}.coverage.bedgraph") \
                for focal_sample_id in sample_ids \
                for chrom in aut_chrom_names \
                for haplotype in ["1" ,"2", "intersection"] \
                for certainty in [0.95]]



# ------------------------------------------------------------------------------------------------------------------------
# 4. Annotate high confidence SNPs according to coverage
#


rule further_annotate_snps:
    input:
        parquet = Path("/lustre/scratch126/casm/team154pc/sl17/03.sperm/02.results/01.read_alignment/01.ccs/04.hifiasm/02.hifiasm_0.19.5-r592/02.chromosome_length_scaffolds/") \
            / "{focal_sample_id}" / "reads" / "{chrom}.high_confidence_snps.parquet",
        haplotagged_bam1 = Path("/lustre/scratch126/casm/team154pc/sl17/03.sperm/02.results/01.read_alignment/01.ccs/04.hifiasm/02.hifiasm_0.19.5-r592/02.chromosome_length_scaffolds/") \
            / "{focal_sample_id}" / "{focal_sample_id}.{chrom}.hap1.minimap2.sorted.primary_alignments.hifiasm_haplotagged_{certainty}.bam",
        haplotagged_bam_bai1 = Path("/lustre/scratch126/casm/team154pc/sl17/03.sperm/02.results/01.read_alignment/01.ccs/04.hifiasm/02.hifiasm_0.19.5-r592/02.chromosome_length_scaffolds/") \
            / "{focal_sample_id}" / "{focal_sample_id}.{chrom}.hap1.minimap2.sorted.primary_alignments.hifiasm_haplotagged_{certainty}.bam.bai",
        haplotagged_bam2 = Path("/lustre/scratch126/casm/team154pc/sl17/03.sperm/02.results/01.read_alignment/01.ccs/04.hifiasm/02.hifiasm_0.19.5-r592/02.chromosome_length_scaffolds/") \
            / "{focal_sample_id}" / "{focal_sample_id}.{chrom}.hap2.minimap2.sorted.primary_alignments.hifiasm_haplotagged_{certainty}.bam",
        haplotagged_bam_bai2 = Path("/lustre/scratch126/casm/team154pc/sl17/03.sperm/02.results/01.read_alignment/01.ccs/04.hifiasm/02.hifiasm_0.19.5-r592/02.chromosome_length_scaffolds/") \
            / "{focal_sample_id}" / "{focal_sample_id}.{chrom}.hap2.minimap2.sorted.primary_alignments.hifiasm_haplotagged_{certainty}.bam.bai",
        bedgraph1 = Path("/lustre/scratch126/casm/team154pc/sl17/03.sperm/02.results/01.read_alignment/01.ccs/04.hifiasm/02.hifiasm_0.19.5-r592/02.chromosome_length_scaffolds/") \
            / "{focal_sample_id}" / "{focal_sample_id}.{chrom}.hap1.minimap2.sorted.primary_alignments.hifiasm_haplotagged_{certainty}.coverage.bedgraph",
        bedgraph2 = Path("/lustre/scratch126/casm/team154pc/sl17/03.sperm/02.results/01.read_alignment/01.ccs/04.hifiasm/02.hifiasm_0.19.5-r592/02.chromosome_length_scaffolds/") \
            / "{focal_sample_id}" / "{focal_sample_id}.{chrom}.hap2.minimap2.sorted.primary_alignments.hifiasm_haplotagged_{certainty}.coverage.bedgraph",
    output:
        parquet = Path("/lustre/scratch126/casm/team154pc/sl17/03.sperm/02.results/01.read_alignment/01.ccs/04.hifiasm/02.hifiasm_0.19.5-r592/02.chromosome_length_scaffolds/") \
            / "{focal_sample_id}" / "reads" / "{chrom}.certainty_{certainty}.snps.parquet",
    resources:
        mem_mb = 16000,
    run:
        # Create SNPs dataframe
        snps_df = pl.read_parquet(input.parquet)

        # Add phased coverage 
        hap_and_certainty_to_bedgraph = {}
        hap_and_certainty_to_bedgraph[(1, wildcards.certainty)] = input.bedgraph1
        hap_and_certainty_to_bedgraph[(2, wildcards.certainty)] = input.bedgraph2
        annotated_snps_df = diagnostics.add_phasing_coverage_annotation(
            snps_df,
            hap_and_certainty_to_bedgraph,
            pl.col("is_interesting_event"),
        )

        # Add allele coverage annotation
        annotated_snps_df = diagnostics.add_allele_coverage_annotation(
            annotated_snps_df,
            (
                (pl.col("mapq1") >= 60) & \
                (pl.col("mapq2") >= 60) & \
                (pl.col("is_forward1") == pl.col("is_forward2")) & \
                (pl.col("total_mismatches") <= 100) & \
                (pl.col("total_clipping") <= 10)
            )
        )
        print("Done add_allele_coverage_annotation")


        # Add high-quality status annotation
        annotated_snps_df = diagnostics.add_high_quality_annotation(
            annotated_snps_df, 
            input_column_prefix = "is_high_conf",
            output_column_prefix = "is_high_quality",
            phased_coverage_min = 3,
            allele_coverage_min = 3,
        )

        # Add mid-quality status annotation
        annotated_snps_df = diagnostics.add_high_quality_annotation(
            annotated_snps_df, 
            input_column_prefix = "is_mid_conf",
            output_column_prefix = "is_mid_quality",
            phased_coverage_min = 3,
            allele_coverage_min = 3,
        )

        annotated_snps_df.write_parquet(output.parquet)        

rule further_annotate_snps_final:
    input:
        parquet = [str(Path("/lustre/scratch126/casm/team154pc/sl17/03.sperm/02.results/01.read_alignment/01.ccs/04.hifiasm/02.hifiasm_0.19.5-r592/02.chromosome_length_scaffolds/") \
                    / f"{focal_sample_id}" / "reads" / f"{chrom}_RagTag.certainty_0.95.snps.parquet") \
            for focal_sample_id in sample_ids[:1] \
            for chrom in aut_chrom_names[:1]] 


rule coverage_for_all_events:
    input:
        events_parquet = Path("/lustre/scratch126/casm/team154pc/sl17/03.sperm/02.results/01.read_alignment/01.ccs/04.hifiasm/02.hifiasm_0.19.5-r592/02.chromosome_length_scaffolds/") \
            / "{focal_sample_id}" / "reads" / "{chrom}.read_refinement.parquet",
        candidates_parquet = Path("/lustre/scratch126/casm/team154pc/sl17/03.sperm/02.results/01.read_alignment/01.ccs/04.hifiasm/02.hifiasm_0.19.5-r592/02.chromosome_length_scaffolds/") \
            / "{focal_sample_id}" / "reads" / "{chrom}.certainty_{certainty}.candidate_reads.parquet",
        bedgraph = Path("/lustre/scratch126/casm/team154pc/sl17/03.sperm/02.results/01.read_alignment/01.ccs/04.hifiasm/02.hifiasm_0.19.5-r592/02.chromosome_length_scaffolds/") \
            / "{focal_sample_id}" / "{focal_sample_id}.{chrom}.hap{haplotype}.minimap2.sorted.primary_alignments.hifiasm_haplotagged_{certainty}.coverage.bedgraph",
    output:
        cov_bed = Path("/lustre/scratch126/casm/team154pc/sl17/03.sperm/02.results/01.read_alignment/01.ccs/04.hifiasm/02.hifiasm_0.19.5-r592/02.chromosome_length_scaffolds/") \
            / "{focal_sample_id}" / "reads" / "{chrom}.certainty_{certainty}.candidate_reads.hap{haplotype}.coverage_refinement.bed",
        intersect_bed = Path("/lustre/scratch126/casm/team154pc/sl17/03.sperm/02.results/01.read_alignment/01.ccs/04.hifiasm/02.hifiasm_0.19.5-r592/02.chromosome_length_scaffolds/") \
            / "{focal_sample_id}" / "reads" / "{chrom}.certainty_{certainty}.candidate_reads.hap{haplotype}.intersect.bed",
        cov_parquet = Path("/lustre/scratch126/casm/team154pc/sl17/03.sperm/02.results/01.read_alignment/01.ccs/04.hifiasm/02.hifiasm_0.19.5-r592/02.chromosome_length_scaffolds/") \
            / "{focal_sample_id}" / "reads" / "{chrom}.certainty_{certainty}.candidate_reads.hap{haplotype}.coverage_refinement.parquet",
    resources:
        mem_mb = 16000,
    run:
        coverage_col = f"min_coverage_hap{wildcards.haplotype}"
        events_df = pl.scan_parquet(input.events_parquet)
        candidates_df = pl.scan_parquet(input.candidates_parquet)
        
        # Take just the subset of events that belong to candidate reads
        cov_df = (events_df
            .select(["read_name", "start", "end", f"ref{wildcards.haplotype}_start", f"ref{wildcards.haplotype}_end"])
            .join(
                candidates_df.lazy().select("read_name"), 
                on="read_name",
            )
        ).collect()

        # If this is empty (no candidate reads), output a default dataframe
        if len(cov_df) == 0:
            shell("touch {output.cov_bed}")
            shell("touch {output.intersect_bed}")
            cov_df = pl.DataFrame(schema={"start": pl.Int64, "end": pl.Int64, "read_name": pl.String, coverage_col: pl.Int64})

        else:
            # Write a bed file for processing with bedtools
            (cov_df
                .select([
                    pl.lit(wildcards.chrom).alias("chrom"), 
                    pl.col(f"ref{wildcards.haplotype}_start"),
                    pl.col(f"ref{wildcards.haplotype}_end"),
                    "read_name",
                ])
                .filter(pl.col(f"ref{wildcards.haplotype}_start") < pl.col(f"ref{wildcards.haplotype}_end"))
                .write_csv(
                    output.cov_bed,
                    separator = "\t",
                    include_header = False,
                )
            )

            # Find the intersection to the coverage; set the coverage to 0 artifically when there is no overlap
            shell(
                "{bedtools_path} intersect "
                "-a {output.cov_bed} "
                "-b {input.bedgraph} "
                "-wao -bed | "
                "awk -v OFS='\t' -F'\t' '{{ if ($9 == 0) $8 = 0; print }}' | "
                "{bedtools_path} groupby -i stdin -g 1,2,3,4 -c 8 -o min "
                " > {output.intersect_bed}"
            )
            
            # Add to the events        
            cov_df = (cov_df
                .join(
                    pl.read_csv(
                        output.intersect_bed,            
                        new_columns = ["chrom", f"ref{wildcards.haplotype}_start", f"ref{wildcards.haplotype}_end", "read_name", coverage_col],
                        has_header = False,
                        separator = "\t",
                    ),
                    on=["read_name", f"ref{wildcards.haplotype}_start", f"ref{wildcards.haplotype}_end"],
                    how="left",
                )            
                .select(["start", "end", "read_name", coverage_col])
                .with_columns(
                    pl.when(pl.col("start") == pl.col("end"))
                        .then(pl.lit(None))
                        .otherwise(pl.col(coverage_col)).alias(coverage_col)
                )        
            )

        
        # Write the output
        cov_df.write_parquet(output.cov_parquet)





rule coverage_for_all_events_all:
    input:
        cov_bed = [str(Path("/lustre/scratch126/casm/team154pc/sl17/03.sperm/02.results/01.read_alignment/01.ccs/04.hifiasm/02.hifiasm_0.19.5-r592/02.chromosome_length_scaffolds/") \
            / f"{focal_sample_id}" / "reads" / f"{chrom}_RagTag.certainty_{certainty}.candidate_reads.hap{haplotype}.coverage_refinement.bed") 
                for focal_sample_id in ["PD50489e"] \
                for chrom in ["chr2"] \
                for haplotype in [1,2] \
                for certainty in [0.95]]


rule coverage_for_all_reads:
    input:
        events_parquet = Path("/lustre/scratch126/casm/team154pc/sl17/03.sperm/02.results/01.read_alignment/01.ccs/04.hifiasm/02.hifiasm_0.19.5-r592/02.chromosome_length_scaffolds/") \
            / "{focal_sample_id}" / "reads" / "{chrom}.read_refinement.parquet",
        bedgraph = Path("/lustre/scratch126/casm/team154pc/sl17/03.sperm/02.results/01.read_alignment/01.ccs/04.hifiasm/02.hifiasm_0.19.5-r592/02.chromosome_length_scaffolds/") \
            / "{focal_sample_id}" / "{focal_sample_id}.{chrom}.hap{haplotype}.minimap2.sorted.primary_alignments.hifiasm_haplotagged_{certainty}.coverage.bedgraph",
    output:
        cov_bed = Path("/lustre/scratch126/casm/team154pc/sl17/03.sperm/02.results/01.read_alignment/01.ccs/04.hifiasm/02.hifiasm_0.19.5-r592/02.chromosome_length_scaffolds/") \
            / "{focal_sample_id}" / "reads" / "{chrom}.certainty_{certainty}.all_reads.hap{haplotype}.coverage_refinement.bed",
        intersect_bed = Path("/lustre/scratch126/casm/team154pc/sl17/03.sperm/02.results/01.read_alignment/01.ccs/04.hifiasm/02.hifiasm_0.19.5-r592/02.chromosome_length_scaffolds/") \
            / "{focal_sample_id}" / "reads" / "{chrom}.certainty_{certainty}.all_reads.hap{haplotype}.intersect.bed",
        cov_parquet = Path("/lustre/scratch126/casm/team154pc/sl17/03.sperm/02.results/01.read_alignment/01.ccs/04.hifiasm/02.hifiasm_0.19.5-r592/02.chromosome_length_scaffolds/") \
            / "{focal_sample_id}" / "reads" / "{chrom}.certainty_{certainty}.all_reads.hap{haplotype}.coverage_refinement.parquet",
    resources:
        mem_mb = 16000,
    run:
        coverage_col = f"min_coverage_hap{wildcards.haplotype}"
        events_df = pl.scan_parquet(input.events_parquet)
        
        # Take the read starts and ends
        cov_df = (events_df
            .select(["read_name", f"ref{wildcards.haplotype}_start", f"ref{wildcards.haplotype}_end"])
            .group_by("read_name")
            .agg(
                pl.col(f"ref{wildcards.haplotype}_start").min(),
                pl.col(f"ref{wildcards.haplotype}_end").max(),
            )
        ).collect()

        # Write a bed file for processing with bedtools
        (cov_df
            .select([
                pl.lit(wildcards.chrom).alias("chrom"), 
                pl.col(f"ref{wildcards.haplotype}_start"),
                pl.col(f"ref{wildcards.haplotype}_end"),
                "read_name",
            ])
            .filter(pl.col(f"ref{wildcards.haplotype}_start") < pl.col(f"ref{wildcards.haplotype}_end"))
            .write_csv(
                output.cov_bed,
                separator = "\t",
                include_header = False,
            )
        )

        # Find the intersection to the coverage; set the coverage to 0 artifically when there is no overlap
        shell(
            "{bedtools_path} intersect "
            "-a {output.cov_bed} "
            "-b {input.bedgraph} "
            "-wao -bed | "
            "awk -v OFS='\t' -F'\t' '{{ if ($9 == 0) $8 = 0; print }}' | "
            "{bedtools_path} groupby -i stdin -g 1,2,3,4 -c 8 -o min "
            " > {output.intersect_bed}"
        )
        
        # Write to parquet
        pl.read_csv(
            output.intersect_bed,            
            new_columns = ["chrom", f"ref{wildcards.haplotype}_start", f"ref{wildcards.haplotype}_end", "read_name", coverage_col],
            has_header = False,
            separator = "\t",
        ).write_parquet(output.cov_parquet)

rule coverage_for_all_reads_all:
    input:
        cov_bed = [str(Path("/lustre/scratch126/casm/team154pc/sl17/03.sperm/02.results/01.read_alignment/01.ccs/04.hifiasm/02.hifiasm_0.19.5-r592/02.chromosome_length_scaffolds/") \
            / f"{focal_sample_id}" / "reads" / f"{chrom}_RagTag.certainty_{certainty}.all_reads.hap{haplotype}.coverage_refinement.bed") 
                for focal_sample_id in sample_ids \
                for chrom in aut_chrom_names \
                for haplotype in [1,2] \
                for certainty in [0.95]]



# ------------------------------------------------------------------------------------------------------------------------
# 5. Generate candidate reads + dashboard
#
rule find_candidate_reads:
    input:
        parquet = Path("/lustre/scratch126/casm/team154pc/sl17/03.sperm/02.results/01.read_alignment/01.ccs/04.hifiasm/02.hifiasm_0.19.5-r592/02.chromosome_length_scaffolds/") \
            / "{focal_sample_id}" / "reads" / "{chrom}.certainty_{certainty}.snps.parquet",
    output:
        all_parquet = Path("/lustre/scratch126/casm/team154pc/sl17/03.sperm/02.results/01.read_alignment/01.ccs/04.hifiasm/02.hifiasm_0.19.5-r592/02.chromosome_length_scaffolds/") \
            / "{focal_sample_id}" / "reads" / "{chrom}.certainty_{certainty}.all_reads.parquet",
        candidates_parquet = Path("/lustre/scratch126/casm/team154pc/sl17/03.sperm/02.results/01.read_alignment/01.ccs/04.hifiasm/02.hifiasm_0.19.5-r592/02.chromosome_length_scaffolds/") \
            / "{focal_sample_id}" / "reads" / "{chrom}.certainty_{certainty}.candidate_reads.parquet",
    resources:
        mem_mb = 4000,
    run:
        annotated_snps_df = pl.scan_parquet(input.parquet)

        # First make stats for all reads, just high-confidence but not high-quality SNPs, like in the phasing coverage
        all_stats_df = diagnostics.snps_to_read_stats(
            annotated_snps_df,
            pl.col("is_high_conf_snp"),
            "frac_fits1_more_snps_high_conf",    
        )

        # Add some read-specific information
        all_stats_df = all_stats_df.join(
            (annotated_snps_df
                .select("read_name", "mapq1", "mapq2", "is_forward1", "is_forward2", "total_mismatches", "num_common_insertions", "num_common_deletions", "total_clipping")
                .unique()
            ),
            on="read_name",
            how="left",
        )
        all_stats_df.sink_parquet(output.all_parquet)

        # Then just for candidates
        hap_stats_df = diagnostics.snps_to_read_stats(
            annotated_snps_df,
            pl.col("is_high_quality_snp"),
            "frac_fits1_super_conf",    
        )
        hap_stats_df = hap_stats_df.filter(~((pl.col("frac_fits1_super_conf") == 0) | (pl.col("frac_fits1_super_conf") == 1)))

        hap_stats_df.sink_parquet(output.candidates_parquet)

rule find_candidate_reads_final:
    input:
        parquet = [str(Path("/lustre/scratch126/casm/team154pc/sl17/03.sperm/02.results/01.read_alignment/01.ccs/04.hifiasm/02.hifiasm_0.19.5-r592/02.chromosome_length_scaffolds/") \
                    / f"{focal_sample_id}" / "reads" / f"{chrom}_RagTag.certainty_{certainty}.candidate_reads.parquet") \
            for focal_sample_id in ["PD50489e"] \
            for certainty in [0.95] \
            for chrom in ["chr2"]]

rule create_dashboard_bams:
    input:
        all_reads_parquet = Path("/lustre/scratch126/casm/team154pc/sl17/03.sperm/02.results/01.read_alignment/01.ccs/04.hifiasm/02.hifiasm_0.19.5-r592/02.chromosome_length_scaffolds/") \
            / "{focal_sample_id}" / "reads" / "{chrom}.certainty_0.95.all_reads.parquet",
        candidate_reads_parquet = Path("/lustre/scratch126/casm/team154pc/sl17/03.sperm/02.results/01.read_alignment/01.ccs/04.hifiasm/02.hifiasm_0.19.5-r592/02.chromosome_length_scaffolds/") \
            / "{focal_sample_id}" / "reads" / "{chrom}.certainty_0.95.candidate_reads.parquet",
        denovo_hap1_alignment_bam_file = Path("/lustre/scratch126/casm/team154pc/sl17/03.sperm/02.results/01.read_alignment/01.ccs/04.hifiasm/02.hifiasm_0.19.5-r592/02.chromosome_length_scaffolds/") \
            / "{focal_sample_id}" / "{focal_sample_id}.hap1.minimap2.sorted.primary_alignments.bam",
        denovo_hap2_alignment_bam_file = Path("/lustre/scratch126/casm/team154pc/sl17/03.sperm/02.results/01.read_alignment/01.ccs/04.hifiasm/02.hifiasm_0.19.5-r592/02.chromosome_length_scaffolds/") \
            / "{focal_sample_id}" / "{focal_sample_id}.hap2.minimap2.sorted.primary_alignments.bam",
    output:
        plots_dir = directory(Path("/lustre/scratch126/casm/team154pc/sl17/03.sperm/02.results/01.read_alignment/01.ccs/04.hifiasm/02.hifiasm_0.19.5-r592/02.chromosome_length_scaffolds/") \
            / "{focal_sample_id}" / "reads" / "plots" / "{chrom}"),
    run:
        dashboard.write_all_bams(
            input.all_reads_parquet,
            input.candidate_reads_parquet,
            input.denovo_hap1_alignment_bam_file,
            output.plots_dir,
            "hap1.bam",
            wildcards.chrom,
            "frac_fits1_more_snps_high_conf",
        )

        dashboard.write_all_bams(
            input.all_reads_parquet,
            input.candidate_reads_parquet,
            input.denovo_hap2_alignment_bam_file,
            output.plots_dir,
            "hap2.bam",
            wildcards.chrom,
            "frac_fits1_more_snps_high_conf",
        )

rule create_dashboard_bams_final:
    input:
        plots_dir = [str(Path("/lustre/scratch126/casm/team154pc/sl17/03.sperm/02.results/01.read_alignment/01.ccs/04.hifiasm/02.hifiasm_0.19.5-r592/02.chromosome_length_scaffolds/") \
            / f"{focal_sample_id}" / "reads" / "plots" / f"{chrom}_RagTag")
            for focal_sample_id in sample_ids  #["PD50489e"]
            for chrom in aut_chrom_names]
    
# ------------------------------------------------------------------------------------------------------------------------
# 6. Classify candidate reads
#
rule classify_reads:
    input:
        events_parquet = Path("/lustre/scratch126/casm/team154pc/sl17/03.sperm/02.results/01.read_alignment/01.ccs/04.hifiasm/02.hifiasm_0.19.5-r592/02.chromosome_length_scaffolds/") \
            / "{focal_sample_id}" / "reads" / "{chrom}.certainty_{certainty}.snps.parquet",
        candidate_reads_parquet = Path("/lustre/scratch126/casm/team154pc/sl17/03.sperm/02.results/01.read_alignment/01.ccs/04.hifiasm/02.hifiasm_0.19.5-r592/02.chromosome_length_scaffolds/") \
            / "{focal_sample_id}" / "reads" / "{chrom}.certainty_{certainty}.candidate_reads.parquet",    
        cov1_parquet = Path("/lustre/scratch126/casm/team154pc/sl17/03.sperm/02.results/01.read_alignment/01.ccs/04.hifiasm/02.hifiasm_0.19.5-r592/02.chromosome_length_scaffolds/") \
            / "{focal_sample_id}" / "reads" / "{chrom}.certainty_{certainty}.candidate_reads.hap1.coverage_refinement.parquet",
        cov2_parquet = Path("/lustre/scratch126/casm/team154pc/sl17/03.sperm/02.results/01.read_alignment/01.ccs/04.hifiasm/02.hifiasm_0.19.5-r592/02.chromosome_length_scaffolds/") \
            / "{focal_sample_id}" / "reads" / "{chrom}.certainty_{certainty}.candidate_reads.hap2.coverage_refinement.parquet",
    output:
        classified_reads_parquet = Path("/lustre/scratch126/casm/team154pc/sl17/03.sperm/02.results/01.read_alignment/01.ccs/04.hifiasm/02.hifiasm_0.19.5-r592/02.chromosome_length_scaffolds/") \
            / "{focal_sample_id}" / "reads" / "{chrom}.certainty_{certainty}.classified_reads.parquet",   
    resources: 
        mem_mb = 4000,
    run:
        # TODO: Parameterize better
        basic_filtering = \
            (~pl.col("has_common_transition")) & \
            (pl.col("min_coverage_between_transitions_hap1") >= 3) & \
            (pl.col("min_coverage_between_transitions_hap2") >= 3) & \
            (pl.col("mapq1") >= 60) & \
            (pl.col("mapq2") >= 60) & \
            (pl.col("is_forward1") == pl.col("is_forward2"))

        extra_filtering = \
            (pl.col("total_mismatches") <= 100) & \
            (pl.col("total_clipping") <= 10) & \
            (~pl.col("read_name").is_in(blacklists.NCO_read_blacklist))
            
        res_df = diagnostics.classify_all_reads(
            pl.read_parquet(input.events_parquet),
            pl.read_parquet(input.candidate_reads_parquet),
            pl.read_parquet(input.cov1_parquet),
            pl.read_parquet(input.cov2_parquet),
            basic_filtering & extra_filtering,           
        )
    
        res_df = res_df.with_columns(
            pl.lit(wildcards.chrom).alias("chrom"), 
            pl.lit(wildcards.focal_sample_id).alias("sample_id")
        )

        res_df.write_parquet(output.classified_reads_parquet)

rule classify_reads_final:
    input:
        plots_dir = [str(Path("/lustre/scratch126/casm/team154pc/sl17/03.sperm/02.results/01.read_alignment/01.ccs/04.hifiasm/02.hifiasm_0.19.5-r592/02.chromosome_length_scaffolds/") \
            / f"{focal_sample_id}" / "reads" / f"{chrom}_RagTag.certainty_0.95.classified_reads.parquet")
            for focal_sample_id in sample_ids
            for chrom in aut_chrom_names]

# ------------------------------------------------------------------------------------------------------------------------
# 7. Annotate all reads structure for analysis
#
rule annotate_all_reads_structure:
    input:
        parquet = Path("/lustre/scratch126/casm/team154pc/sl17/03.sperm/02.results/01.read_alignment/01.ccs/04.hifiasm/02.hifiasm_0.19.5-r592/02.chromosome_length_scaffolds/") \
            / "{focal_sample_id}" / "reads" / "{chrom}.certainty_{certainty}.snps.parquet",
        csv_grch37 = Path("/lustre/scratch126/casm/team154pc/sl17/03.sperm/02.results/01.read_alignment/01.ccs/01.grch37/") \
            / "{focal_sample_id}" / "{focal_sample_id}.minimap2.sorted.primary_alignments.ref_starts.csv.gz",            
        csv_grch38 = Path("/lustre/scratch126/casm/team154pc/sl17/03.sperm/02.results/01.read_alignment/01.ccs/02.grch38/") \
            / "{focal_sample_id}" / "{focal_sample_id}.minimap2.sorted.primary_alignments.ref_starts.csv.gz",
        csv_T2T = Path("/lustre/scratch126/casm/team154pc/sl17/03.sperm/02.results/01.read_alignment/01.ccs/03.T2T-CHM13/") \
            / "{focal_sample_id}" / "{focal_sample_id}.minimap2.sorted.primary_alignments.ref_starts.csv.gz",
        cov_hap1_parquet = Path("/lustre/scratch126/casm/team154pc/sl17/03.sperm/02.results/01.read_alignment/01.ccs/04.hifiasm/02.hifiasm_0.19.5-r592/02.chromosome_length_scaffolds/") \
            / "{focal_sample_id}" / "reads" / "{chrom}.certainty_{certainty}.all_reads.hap1.coverage_refinement.parquet",
        cov_hap2_parquet = Path("/lustre/scratch126/casm/team154pc/sl17/03.sperm/02.results/01.read_alignment/01.ccs/04.hifiasm/02.hifiasm_0.19.5-r592/02.chromosome_length_scaffolds/") \
            / "{focal_sample_id}" / "reads" / "{chrom}.certainty_{certainty}.all_reads.hap2.coverage_refinement.parquet",    
        AA_hotspots = "/lustre/scratch126/casm/team154pc/sl17/03.sperm/01.data/07.references/06.hotspots/hinch_2023_AA_hotspots.csv",
        CL4_hotspots = "/lustre/scratch126/casm/team154pc/sl17/03.sperm/01.data/07.references/06.hotspots/hinch_2023_CL4_hotspots.csv",            
    output:
        all_parquet = Path("/lustre/scratch126/casm/team154pc/sl17/03.sperm/02.results/01.read_alignment/01.ccs/04.hifiasm/02.hifiasm_0.19.5-r592/02.chromosome_length_scaffolds/") \
            / "{focal_sample_id}" / "reads" / "{chrom}.certainty_{certainty}.all_reads_structure_annotated.parquet",
    resources:
        time_min = 10,
        mem_mb = 8000,
    run:
        reads_df = annotate.annotate_read_structure(
            input.parquet,
            wildcards.focal_sample_id,
            wildcards.chrom.replace("_RagTag", ""),
            input.csv_grch37,
            input.csv_grch38,
            input.csv_T2T,
            input.cov_hap1_parquet,
            input.cov_hap2_parquet,
            input.AA_hotspots,
            input.CL4_hotspots,
            GC_tract_mean = 30,
            min_mapq = 60,
            max_total_mismatches = 100,
            max_total_clipping = 10,
        )

        reads_df.write_parquet(output.all_parquet)

rule annotate_all_reads_structure_final:
    input:
        all_parquet = [str(Path("/lustre/scratch126/casm/team154pc/sl17/03.sperm/02.results/01.read_alignment/01.ccs/04.hifiasm/02.hifiasm_0.19.5-r592/02.chromosome_length_scaffolds/") \
            / f"{focal_sample_id}" / "reads" / f"{chrom}_RagTag.certainty_0.95.all_reads_structure_annotated.parquet") \
            for focal_sample_id in sample_ids
            for chrom in aut_chrom_names]