import pickle
import numba
import joblib

rule infer_tract_length:
    output:
        pcl = "/lustre/scratch122/tol/projects/sperm/results/tract_length_inference_sperm_20250330/inference.take_every={take_every}.bootstrap={bootstrap}.rep={rep}.pcl",
    resources:
        mem_mb = 256*1000,
        time_min = 120,
    threads: 32,
    run:
        rahbari_df = pl.read_csv("/nfs/users/nfs_r/rs42/rs42/git/sperm/configs/Rahbari.tsv", separator='\t')
        sudmant_df = (
            pl.read_csv("/nfs/users/nfs_r/rs42/rs42/git/sperm/configs/Sudmant.tsv", separator='\t')
            .with_columns(
                pl.col("sample_set").cast(pl.String),
                pl.col("sample_id").cast(pl.String),
            )
        )
        reads_df_list = (
            [
                pl.scan_parquet(
                    f"/lustre/scratch122/tol/projects/sperm/results/Rahbari_20250212/read_analysis/{sample_set}/{sample_id}/reads/{chrom}/all_reads_structure_annotated.parquet",
                ) 
                for sample_id, sample_set in (rahbari_df.select("sample_id", "sample_set").unique().iter_rows())
                for chrom in aut_chrom_names
            ] +
            [
                pl.scan_parquet(
                    f"/lustre/scratch122/tol/projects/sperm/results/Sudmant_20241121/read_analysis/{sample_set}/{sample_id}/reads/{chrom}/all_reads_structure_annotated.parquet",
                ) 
                for sample_id, sample_set in (sudmant_df.select("sample_id", "sample_set").unique().iter_rows())
                for chrom in aut_chrom_names
            ]
        )

        sperm_sample_ids_except_AD = [x for x in IDs.rahbari_sample_ids + IDs.sudmant_sample_ids if x != 'PD50523b']


        if wildcards.bootstrap == "1":
            bootstrap = True
        else:
            bootstrap = False

        rep = int(wildcards.rep)
        take_every = int(wildcards.take_every)

        reads_df = pl.concat(reads_df_list)

        idf = inference.generate_call_set(
            reads_df,
            sperm_sample_ids_except_AD, 
            take_every=take_every, 
            bootstrap=bootstrap, 
            min_snps=3, 
            sample_every=1,
        )


        res = inference.maximum_likelihood_all_reads(
            idf["read_length"].to_numpy(),
            idf["high_quality_snp_positions"].to_numpy(),
            idf["high_quality_snps_idx_transitions"].to_numpy(),
            idf["between_high_quality_snps_cM"].to_numpy() * 1e-2,
            idf["before_read_cM"].to_numpy() * 1e-2,
            idf["after_read_cM"].to_numpy() * 1e-2,
            idf["weight"].to_numpy(),
            q_range = (0.05, 0.5),   
            m_range = (0.8, 1),
            GC_tract_mean_range = (5, 1000),
            GC_tract_mean2_range = (100, 100000),
            prob_factor_range = (1,1),
            read_margin_in_bp = 5000,
            x0 = [0.1, 0.98, 30, 1000, 1],
        )


        pickle.dump(res, open(output.pcl, "wb"))

rule infer_tract_length_final:
    input:
        pcl = [f"/lustre/scratch122/tol/projects/sperm/results/tract_length_inference_sperm_20250330/inference.take_every={take_every}.bootstrap={bootstrap}.rep={rep}.pcl"
            for take_every, bootstrap, rep in \
                [[10, 0, 0]] + [[10, 1, i] for i in range(100)]
        ]

# #### TODO CHANGE, very hacky
# ceph_good_samples = "NA12878,NA12879,NA12881,NA12882,NA12886,NA12892,200084,200085,200102,200104".split(',')
# rule infer_joint_tract_length:
#     input:
#         sperm_reads = [str(Path("/lustre/scratch126/casm/team154pc/sl17/03.sperm/02.results/01.read_alignment/01.ccs/04.hifiasm/02.hifiasm_0.19.5-r592/02.chromosome_length_scaffolds") \
#             / f"{focal_sample_id}" / "reads" / f"{chrom}_RagTag.certainty_0.95.all_reads_structure_annotated.parquet") \
#                 for focal_sample_id in IDs.rahbari_sample_ids \
#                 for chrom in aut_chrom_names],
#         blood_reads = [str(output_path / f"read_analysis/{sample_id}/{sample_id}/reads/{chrom}/all_reads_structure_annotated.parquet")
#             for sample_id in ceph_good_samples
#             for chrom in aut_chrom_names
#         ]
#     output:
#         pcl = "/lustre/scratch126/casm/team154pc/sl17/03.sperm/02.results/08.tract_length/joint_inference.sample_every={sample_every}.bootstrap={bootstrap}.rep={rep}.pcl",
#     resources:
#         mem_mb = 256*1000,
#         time_min = 60*5,
#     threads: 32,
#     run:
#         if wildcards.bootstrap == "1":
#             bootstrap = True
#         else:
#             bootstrap = False

#         rep = int(wildcards.rep)
#         sample_every = int(wildcards.sample_every)

#         blood_reads_df = pl.scan_parquet(input.blood_reads)
#         blood_callset_df = inference.generate_call_set(
#             blood_reads_df, 
#             ceph_good_samples, 
#             take_every=10, 
#             min_snps=3, 
#             sample_every=sample_every,
#             bootstrap=bootstrap,
#         )

#         sperm_reads_df = pl.scan_parquet(input.sperm_reads)
#         sperm_callset_df = inference.generate_call_set(
#             sperm_reads_df, 
#             IDs.rahbari_sample_ids, 
#             take_every=1, 
#             min_snps=3, 
#             sample_every=sample_every,
#             bootstrap=bootstrap,
#         )


#         res = inference.maximum_likelihood_all_reads_joint(
#             sperm_callset_df["read_length"].to_numpy(),
#             sperm_callset_df["high_quality_snp_positions"].to_numpy(),
#             sperm_callset_df["high_quality_snps_idx_transitions"].to_numpy(),
#             sperm_callset_df["between_high_quality_snps_cM"].to_numpy() * 1e-2,
#             sperm_callset_df["before_read_cM"].to_numpy() * 1e-2,
#             sperm_callset_df["after_read_cM"].to_numpy() * 1e-2,
#             # sperm_callset_df["between_high_quality_snps_bp"].to_numpy() * 1e-8,
#             # numba.typed.List(np.repeat(5000 * 1e-8, len(sperm_callset_df))),
#             # numba.typed.List(np.repeat(5000 * 1e-8, len(sperm_callset_df))),
#             sperm_callset_df["weight"].to_numpy(),

#             blood_callset_df["read_length"].to_numpy(),
#             blood_callset_df["high_quality_snp_positions"].to_numpy(),
#             blood_callset_df["high_quality_snps_idx_transitions"].to_numpy(),
#             blood_callset_df["between_high_quality_snps_bp"].to_numpy() * 1e-8,
#             numba.typed.List(np.repeat(5000 * 1e-8, len(blood_callset_df))),
#             numba.typed.List(np.repeat(5000 * 1e-8, len(blood_callset_df))),
#             blood_callset_df["weight"].to_numpy(),

#             q_range_sperm = (0.01, 0.5),   
#             q_range_blood = (1e-10, 0.5),   
            
#             #m_range_sperm = (1e-10, 1-1e-10),
#             m_range_sperm = (0.8, 1-1e-10),
#             m_range_blood = (1e-10, 1-1e-10),
            
#             GC_tract_mean_range = (10, 1000),
#             GC_tract_mean2_range = (100, 10000),
            
#             prob_factor_range_sperm = (1, 1),
#             prob_factor_range_blood = (1e-10, 1),
            
#             read_margin_in_bp = 5000,
            
#             x0 = [
#                 0.1, 0.1, 
#                 0.98, 0.01,     
#                 30, 1000, 
#                 1.0, 1e-2,
#             ],
#         )

#         pickle.dump(res, open(output.pcl, "wb"))        

# rule infer_joint_tract_length_final:
#     input:
#         pcl = [f"/lustre/scratch126/casm/team154pc/sl17/03.sperm/02.results/08.tract_length/joint_inference.sample_every={sample_every}.bootstrap={bootstrap}.rep={rep}.pcl"
#             for sample_every, bootstrap, rep in \
#                 [[1, 0, 0]]]

rule simulate_tracts:
    input:
    output:
        parquet = "/lustre/scratch122/tol/projects/sperm/results/tract_length_inference_sperm_20250407/tract_stats_m={m}_L1={L1}_L2={L2}.parquet",
    resources:
        mem_mb = 128*1000,
        time_min = 30,
    threads: 16,
    run:
        rahbari_df = pl.read_csv("/nfs/users/nfs_r/rs42/rs42/git/sperm/configs/Rahbari.tsv", separator='\t')
        sudmant_df = (
            pl.read_csv("/nfs/users/nfs_r/rs42/rs42/git/sperm/configs/Sudmant.tsv", separator='\t')
            .with_columns(
                pl.col("sample_set").cast(pl.String),
                pl.col("sample_id").cast(pl.String),
            )
        )

        reads_filenames = (
            [
                (
                    f"/lustre/scratch122/tol/projects/sperm/results/Rahbari_20250212/read_analysis/{sample_set}/{sample_id}/reads/{chrom}/all_reads_structure_annotated.parquet"
                ) 
                for sample_id, sample_set in (rahbari_df.select("sample_id", "sample_set").unique().iter_rows())
                for chrom in aut_chrom_names
            ] + 
            [
                (
                    f"/lustre/scratch122/tol/projects/sperm/results/Sudmant_20241121/read_analysis/{sample_set}/{sample_id}/reads/{chrom}/all_reads_structure_annotated.parquet"
                ) 
                for sample_id, sample_set in (sudmant_df.select("sample_id", "sample_set").unique().iter_rows())
                for chrom in aut_chrom_names
            ]
        )

        reads_df_list = [pl.scan_parquet(filename) for filename in reads_filenames]

        dfs = joblib.Parallel(n_jobs=-1, verbose=1)(
            joblib.delayed(inference.generate_call_set)(
                reads_df, IDs.rahbari_sample_ids + IDs.sudmant_sample_ids, 10, False, 3, 1
            )
            for reads_df in reads_df_list
        )

        idf = pl.concat([df for df in dfs if len(df)])

        m = float(wildcards.m)
        GC_tract_mean = float(wildcards.L1)
        GC_tract_mean2 = float(wildcards.L2)
            
        recomb_rate_for_sim = 1

        res = inference.simulate_read_patterns_probs(
            numba.typed.List(idf["read_length"].to_numpy()),
            numba.typed.List(idf["high_quality_snp_positions"].to_numpy()),
            numba.typed.List(idf["between_high_quality_snps_bp"].to_numpy() * recomb_rate_for_sim),
            numba.typed.List(np.repeat(5000 * recomb_rate_for_sim, len(idf))),
            numba.typed.List(np.repeat(5000 * recomb_rate_for_sim, len(idf))),
            q = 0,
            m = m,
            GC_tract_mean = GC_tract_mean,
            GC_tract_mean2 = GC_tract_mean2,
            read_margin_in_bp = 5000,
            allow_flip = False,
        )

        xxx = idf["high_quality_snp_positions"].to_numpy()

        sim_stats_df = pl.DataFrame(
            [(
                idx_trans[1] - idx_trans[0],
                xxx[event_index][idx_trans[1]] - xxx[event_index][idx_trans[0]+1],
                xxx[event_index][idx_trans[1]+1] - xxx[event_index][idx_trans[0]],
            ) 
                for idx_trans, event_index, event_type, tract_type, tract_length in zip(res[0], res[1], res[6], res[7], res[8]) \
                if event_type == 0 and len(idx_trans) == 2],
            orient="row",
            schema=["n_converted", "lower_bound", "upper_bound"],
        )

        sim_stats_df.write_parquet(
            output.parquet,
        )

rule simulate_tracts_final:
    input:
        parquet = [
            f"/lustre/scratch122/tol/projects/sperm/results/tract_length_inference_sperm_20250407/tract_stats_m={m:1.3f}_L1={L1:1.3f}_L2={L2:1.3f}.parquet"
            for m in np.linspace(0.97, 1, 11)
            for L1 in np.linspace(10, 100, 11)
            for L2 in np.linspace(500, 2000, 21)
        ]

rule simulate_tracts_final_second_round:
    input:
        parquet = [
            f"/lustre/scratch122/tol/projects/sperm/results/tract_length_inference_sperm_20250407/tract_stats_m={m:1.3f}_L1={L1:1.3f}_L2={L2:1.3f}.parquet"
            for m in np.linspace(0.988, 0.994, 7)
            for L1 in np.linspace(20, 50, 31)
            for L2 in np.linspace(650, 1550, 31)
        ]        

rule simulate_tracts_final_single_component:
    input:
        parquet = [
            f"/lustre/scratch122/tol/projects/sperm/results/tract_length_inference_sperm_20250407/tract_stats_m={m:1.3f}_L1={L1:1.3f}_L2={L2:1.3f}.parquet"
            for m in [1.0]
            for L1 in np.linspace(10, 1000, 500)
            for L2 in [1000]
        ]                