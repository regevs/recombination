# ------------------------------------------------------------------------------------------------------------------------
# 1. Analyze reads
#
rule read_refinement:
    input:
        bam_filename_1 = output_path / "alignments/{sample_set}/{sample_id}/{flow_cell}/T2T_scaffolds/haplotype_1" \
            / "minimap2.sorted.primary_alignments.bam",
        bam_filename_2 = output_path / "alignments/{sample_set}/{sample_id}/{flow_cell}/T2T_scaffolds/haplotype_2" \
            / "minimap2.sorted.primary_alignments.bam",    
    output:
        parquet = output_path / "read_analysis/{sample_set}/{sample_id}/{flow_cell}/{chrom}/read_refinement.parquet",
    threads: 8,
    resources:
        mem_mb=256000,
    run:
        print("Running...")
        cdf = diagnostics.run_all_refine_cigars(
            input.bam_filename_1,
            input.bam_filename_2,
            wildcards.chrom + "_RagTag",
            threads,
        )

        print("Writing...")        
        cdf.write_parquet(
            output.parquet,
        )

rule read_refinement_final:
    input:
        csv_gz = [str(output_path / f"read_analysis/{R.sample_set}/{R.sample_id}/{R.flow_cell}/{chrom}/read_refinement.parquet") \
                for chrom in aut_chrom_names \
                for R in data_rows] 


# ------------------------------------------------------------------------------------------------------------------------
# 2. Call and annotate high confidence SNPs
#
rule create_high_conf_snps:
    input:
        refinement_parquet = output_path / "read_analysis/{sample_set}/{sample_id}/{flow_cell}/{chrom}/read_refinement.parquet",
        trf_dat1 = output_path / "T2T_scaffolds/{sample_set}/{chrom}/haplotype_1/ragtag.scaffold.expanded.fasta.trf.dat",
        trf_dat2 = output_path / "T2T_scaffolds/{sample_set}/{chrom}/haplotype_2/ragtag.scaffold.expanded.fasta.trf.dat",
        sdust_dat1 = output_path / "T2T_scaffolds/{sample_set}/{chrom}/haplotype_1/ragtag.scaffold.expanded.fasta.sdust.dat",
        sdust_dat2 = output_path / "T2T_scaffolds/{sample_set}/{chrom}/haplotype_2/ragtag.scaffold.expanded.fasta.sdust.dat",
    output:
        parquet = output_path / "read_analysis/{sample_set}/{sample_id}/{flow_cell}/{chrom}/high_confidence_snps.parquet",
    resources:
        mem_mb = lambda wildcards, attempt: attempt * 64000,
    run:
        P = flow_cell_to_params[wildcards.flow_cell]

        # Create SNPs dataframe
        events_df = pl.read_parquet(input.refinement_parquet)
        events_df = diagnostics.filter_read_refinements(events_df)

        print("Done filter_read_refinements")

        annotated_events_df = diagnostics.extract_high_conf_events(
            events_df,
            high_confidence_slack = P["high_confidence_slack"],
        )

        print("Done extract_high_conf_events")

        # Add TRF annotations
        annotated_events_df = diagnostics.add_tandem_repeat_finder_annotation(
            annotated_events_df,
            input.trf_dat1,
            input.trf_dat2,
            pl.col("is_flanked_event"),
        )

        print("Done add_tandem_repeat_finder_annotation")

        # Add sdust annotations
        annotated_events_df = diagnostics.add_sdust_annotation(
            annotated_events_df,
            input.sdust_dat1,
            input.sdust_dat2,
            pl.col("is_flanked_event"),
        )

        annotated_events_df = annotated_events_df.collect(streaming=True).lazy()
        print("Done add_sdust_annotation")

        # Add high-quality status annotation
        annotated_events_df = diagnostics.add_high_confidence_annotation(            
            annotated_events_df, 
            output_column_prefix = "is_high_conf",
            base_qual_min = P["base_qual_min_detection"],
            read_trimming = P["read_trimming_detection"],
        )

        # Add mid-quality status annotation
        annotated_events_df = diagnostics.add_high_confidence_annotation(            
            annotated_events_df, 
            output_column_prefix = "is_mid_conf",
            base_qual_min = P["base_qual_min_classification"],
            read_trimming = P["read_trimming_classification"],
        )

        annotated_events_df = annotated_events_df.collect(streaming=True)
        print("Done add_high_confidence_annotation")

        # Write
        annotated_events_df.write_parquet(output.parquet)

rule create_high_conf_snps_all:
    input:
        parquet = [str(output_path / f"read_analysis/{R.sample_set}/{R.sample_id}/{R.flow_cell}/{chrom}/high_confidence_snps.parquet") \
            for R in data_rows
            for chrom in aut_chrom_names] 

# ------------------------------------------------------------------------------------------------------------------------
# 3. Phase and haplotag according to high confidence SNPs
#

rule phase_and_haplotag:
    input:
        parquet = output_path / "read_analysis/{sample_set}/{sample_id}/{flow_cell}/{chrom}/high_confidence_snps.parquet",
        bam1 = output_path / "alignments/{sample_set}/{sample_id}/{flow_cell}/T2T_scaffolds/haplotype_1" \
            / "minimap2.sorted.primary_alignments.bam",
        bam2 = output_path / "alignments/{sample_set}/{sample_id}/{flow_cell}/T2T_scaffolds/haplotype_2" \
            / "minimap2.sorted.primary_alignments.bam",
    output:
        bam1 = output_path / "read_analysis/{sample_set}/{sample_id}/{flow_cell}/{chrom}/haplotype_1/minimap2.sorted.primary_alignments.hifiasm_haplotagged_{certainty}.bam",
        bam_bai1 = output_path / "read_analysis/{sample_set}/{sample_id}/{flow_cell}/{chrom}/haplotype_1/minimap2.sorted.primary_alignments.hifiasm_haplotagged_{certainty}.bam.bai",
        bam2 = output_path / "read_analysis/{sample_set}/{sample_id}/{flow_cell}/{chrom}/haplotype_2/minimap2.sorted.primary_alignments.hifiasm_haplotagged_{certainty}.bam",
        bam_bai2 = output_path / "read_analysis/{sample_set}/{sample_id}/{flow_cell}/{chrom}/haplotype_2/minimap2.sorted.primary_alignments.hifiasm_haplotagged_{certainty}.bam.bai",        
    resources:
        mem_mb=32000,
    run:
        diagnostics.phase_and_haplotag(
            input.parquet,
            input.bam1,
            input.bam2,
            output.bam1,
            output.bam2,
            certainty_threshold=float(wildcards.certainty),
        )

        shell(
            "{samtools_path} index {output.bam1}"
        )

        shell(
            "{samtools_path} index {output.bam2}"
        )

def get_all_flow_cells_bams_for_sample_set(wildcards):
    res = []
    for R in data_rows:
        if R.sample_set == wildcards.sample_set:
            res.append(
                output_path / f"read_analysis/{R.sample_set}/{R.sample_id}/{R.flow_cell}/{wildcards.chrom}" \ 
                    / f"haplotype_{wildcards.haplotype}/minimap2.sorted.primary_alignments.hifiasm_haplotagged_0.95.bam"
            )
    return res

# To calculate coverage, we need to use all the bams for this sample set
rule calculate_genome_coverage_on_haplotype:
    input:
        bams = get_all_flow_cells_bams_for_sample_set,        
    output:
        bedgraph = output_path / "read_analysis/{sample_set}/genome_coverage/{chrom}/haplotype_{haplotype}/hifiasm_haplotagged_{certainty}.coverage.bedgraph",
    resources:
        mem_mb=4000,
        threads=4,
    run:
        shell(
            "{samtools_path} merge -o - -@ {threads} {input.bams} | "
            "{samtools_path} view -@ {threads} --with-header --tag HP:{wildcards.haplotype} - | "
            "{bedtools_path} genomecov -ibam - -bg > {output.bedgraph}"
        )

rule calculate_total_genome_coverage_on_haplotype:
    input:
        bams = get_all_flow_cells_bams_for_sample_set,        
    output:
        bedgraph = output_path / "read_analysis/{sample_set}/genome_coverage/{chrom}/haplotype_{haplotype}/total.coverage.bedgraph",
    resources:
        mem_mb=4000,
        threads=4,
    run:
        shell(
            "{samtools_path} merge -o - -@ {threads} {input.bams} | "
            "{samtools_path} view -@ {threads} --with-header - | "
            "{bedtools_path} genomecov -ibam - -bg > {output.bedgraph}"
        )        


rule phase_and_haplotag_final:
    input:
        bam = [str(output_path / f"read_analysis/{sample_set}/genome_coverage/{chrom}/haplotype_{haplotype}/hifiasm_haplotagged_{certainty}.coverage.bedgraph") \
                for sample_set in sample_sets 
                for chrom in aut_chrom_names
                for haplotype in [1,2] \
                for certainty in [0.95]]



# ------------------------------------------------------------------------------------------------------------------------
# 4. Annotate high confidence SNPs according to coverage
#


rule further_annotate_snps:
    input:
        parquet = output_path / "read_analysis/{sample_set}/{sample_id}/{flow_cell}/{chrom}/high_confidence_snps.parquet",
        all_parquets_of_sample = lambda wc: [
            str(output_path / f"read_analysis/{wc.sample_set}/{wc.sample_id}/{R.flow_cell}/{wc.chrom}/high_confidence_snps.parquet")
                for R in data_rows if R.sample_id == wc.sample_id
        ],
        bedgraph1 = output_path / "read_analysis/{sample_set}/genome_coverage/{chrom}/haplotype_1/hifiasm_haplotagged_{certainty}.coverage.bedgraph",
        bedgraph2 = output_path / "read_analysis/{sample_set}/genome_coverage/{chrom}/haplotype_2/hifiasm_haplotagged_{certainty}.coverage.bedgraph",
        total_bedgraph1 = output_path / "read_analysis/{sample_set}/genome_coverage/{chrom}/haplotype_1/total.coverage.bedgraph",
        total_bedgraph2 = output_path / "read_analysis/{sample_set}/genome_coverage/{chrom}/haplotype_2/total.coverage.bedgraph",
    output:
        parquet = output_path / "read_analysis/{sample_set}/{sample_id}/{flow_cell}/{chrom}/annotated_{certainty}_high_confidence_snps.parquet",
    resources:
        mem_mb = 16000,
    run:
        P = flow_cell_to_params[wildcards.flow_cell]

        # Create SNPs dataframe
        snps_df = pl.read_parquet(input.parquet)

        # Add phased coverage 
        hap_column_bedgraph_list = [
            [1, f"hap1_certainty_{wildcards.certainty}_coverage", input.bedgraph1],
            [2, f"hap2_certainty_{wildcards.certainty}_coverage", input.bedgraph2],
            [1, f"hap1_total_coverage", input.total_bedgraph1],
            [2, f"hap2_total_coverage", input.total_bedgraph2],
        ]

        annotated_snps_df = diagnostics.add_phasing_coverage_annotation(
            snps_df,
            hap_column_bedgraph_list,
            pl.col("is_interesting_event"),
        )

        # Add allele coverage annotation
        annotated_snps_df = diagnostics.add_allele_coverage_annotation(
            annotated_snps_df,
            pl.scan_parquet(input.all_parquets_of_sample),
            (
                (pl.col("mapq1") >= P["map_qual_min"]) & \
                (pl.col("mapq2") >= P["map_qual_min"]) & \
                (pl.col("is_forward1") == pl.col("is_forward2")) & \
                (pl.col("total_mismatches") <= P["total_mismatches_max"]) & \
                (pl.col("total_clipping") <= P["total_clipping_max"])
            )
        )
        print("Done add_allele_coverage_annotation")

        # Add balance p-values
        annotated_snps_df = diagnostics.add_coverage_balance_annotation(
            annotated_snps_df,
            wildcards.certainty,
        )

        # Add high-quality status annotation
        annotated_snps_df = diagnostics.add_high_quality_annotation(
            annotated_snps_df, 
            input_column_prefix = "is_high_conf",
            output_column_prefix = "is_high_quality",
            phased_coverage_min = P["phased_coverage_min"],
            allele_coverage_min = P["allele_coverage_min"],
            balance_p_value_threshold = P["balance_p_value_threshold"],
            unassigned_reads_max = P["unassigned_reads_max"],
        )

        # Add mid-quality status annotation
        annotated_snps_df = diagnostics.add_high_quality_annotation(
            annotated_snps_df, 
            input_column_prefix = "is_mid_conf",
            output_column_prefix = "is_mid_quality",
            phased_coverage_min = P["phased_coverage_min"],
            allele_coverage_min = P["allele_coverage_min"],
            balance_p_value_threshold = P["balance_p_value_threshold"],
            unassigned_reads_max = P["unassigned_reads_max"],
        )

        annotated_snps_df.write_parquet(output.parquet)        

rule further_annotate_snps_final:
    input:
        parquet = [str(output_path / f"read_analysis/{R.sample_set}/{R.sample_id}/{R.flow_cell}/{chrom}/annotated_{certainty}_high_confidence_snps.parquet") \
            for R in data_rows
            for certainty in [0.95]
            for chrom in aut_chrom_names] 



rule coverage_for_all_events:
    input:
        events_parquets = lambda wc: [
            str(output_path / f"read_analysis/{wc.sample_set}/{wc.sample_id}/{R.flow_cell}/{wc.chrom}/read_refinement.parquet")
                for R in data_rows if R.sample_id == wc.sample_id
        ],
        candidates_parquet = output_path / "read_analysis/{sample_set}/{sample_id}/reads/{chrom}/candidate_reads.parquet",
        bedgraph = output_path / "read_analysis/{sample_set}/genome_coverage/{chrom}/haplotype_{haplotype}/hifiasm_haplotagged_0.95.coverage.bedgraph",
    output:
        cov_bed = output_path / "read_analysis/{sample_set}/{sample_id}/reads/{chrom}/haplotype_{haplotype}/candidate_reads.coverage_refinement.bed",
        intersect_bed = output_path / "read_analysis/{sample_set}/{sample_id}/reads/{chrom}/haplotype_{haplotype}/candidate_reads.intersect.bed",
        cov_parquet = output_path / "read_analysis/{sample_set}/{sample_id}/reads/{chrom}/haplotype_{haplotype}/candidate_reads.coverage_refinement.parquet",
    resources:
        mem_mb = 64000,
    run:
        coverage_col = f"min_coverage_hap{wildcards.haplotype}"
        events_df = pl.scan_parquet(input.events_parquets)
        candidates_df = pl.scan_parquet(input.candidates_parquet)
        
        # Take just the subset of events that belong to candidate reads
        cov_df = (events_df
            .select(["read_name", "start", "end", f"ref{wildcards.haplotype}_start", f"ref{wildcards.haplotype}_end"])
            .join(
                candidates_df.lazy().select("read_name"), 
                on="read_name",
            )
        ).collect()

        # If this is empty (no candidate reads), output a default dataframe
        if len(cov_df) == 0:
            shell("touch {output.cov_bed}")
            shell("touch {output.intersect_bed}")
            cov_df = pl.DataFrame(schema={"start": pl.Int64, "end": pl.Int64, "read_name": pl.String, coverage_col: pl.Int64})

        else:
            # Write a bed file for processing with bedtools
            (cov_df
                .select([
                    pl.lit(wildcards.chrom + "_RagTag").alias("chrom"), 
                    pl.col(f"ref{wildcards.haplotype}_start"),
                    pl.col(f"ref{wildcards.haplotype}_end"),
                    "read_name",
                ])
                .filter(pl.col(f"ref{wildcards.haplotype}_start") < pl.col(f"ref{wildcards.haplotype}_end"))
                .write_csv(
                    output.cov_bed,
                    separator = "\t",
                    include_header = False,
                )
            )

            # Find the intersection to the coverage; set the coverage to 0 artifically when there is no overlap
            shell(
                "{bedtools_path} intersect "
                "-a {output.cov_bed} "
                "-b {input.bedgraph} "
                "-wao -bed | "
                "awk -v OFS='\t' -F'\t' '{{ if ($9 == 0) $8 = 0; print }}' | "
                "{bedtools_path} groupby -i stdin -g 1,2,3,4 -c 8 -o min "
                " > {output.intersect_bed}"
            )
            
            # Add to the events        
            cov_df = (cov_df
                .join(
                    pl.read_csv(
                        output.intersect_bed,            
                        new_columns = ["chrom", f"ref{wildcards.haplotype}_start", f"ref{wildcards.haplotype}_end", "read_name", coverage_col],
                        has_header = False,
                        separator = "\t",
                    ),
                    on=["read_name", f"ref{wildcards.haplotype}_start", f"ref{wildcards.haplotype}_end"],
                    how="left",
                )            
                .select(["start", "end", "read_name", coverage_col])
                .with_columns(
                    pl.when(pl.col("start") == pl.col("end"))
                        .then(pl.lit(None))
                        .otherwise(pl.col(coverage_col)).alias(coverage_col)
                )        
            )

        
        # Write the output
        cov_df.write_parquet(output.cov_parquet)


rule coverage_for_all_events_all:
    input:
        cov_parquet = [str(output_path / f"read_analysis/{R.sample_set}/{R.sample_id}/reads/{chrom}/haplotype_{haplotype}/candidate_reads.coverage_refinement.parquet") 
                for R in data_rows
                for chrom in aut_chrom_names
                for haplotype in [1,2]]


rule coverage_for_all_reads:
    input:
        events_parquets = lambda wc: [
            str(output_path / f"read_analysis/{wc.sample_set}/{wc.sample_id}/{R.flow_cell}/{wc.chrom}/read_refinement.parquet")
                for R in data_rows if R.sample_id == wc.sample_id
        ],
        bedgraph = output_path / "read_analysis/{sample_set}/genome_coverage/{chrom}/haplotype_{haplotype}/hifiasm_haplotagged_0.95.coverage.bedgraph",
    output:
        cov_bed = output_path / "read_analysis/{sample_set}/{sample_id}/reads/{chrom}/haplotype_{haplotype}/all_reads.coverage_refinement.bed",
        intersect_bed = output_path / "read_analysis/{sample_set}/{sample_id}/reads/{chrom}/haplotype_{haplotype}/all_reads.intersect.bed",
        cov_parquet = output_path / "read_analysis/{sample_set}/{sample_id}/reads/{chrom}/haplotype_{haplotype}/all_reads.coverage_refinement.parquet",
    resources:
        mem_mb = 64000,
    run:
        coverage_col = f"min_coverage_hap{wildcards.haplotype}"
        events_df = pl.scan_parquet(input.events_parquets)
        
        # Take the read starts and ends
        cov_df = (events_df
            .select(["read_name", f"ref{wildcards.haplotype}_start", f"ref{wildcards.haplotype}_end"])
            .group_by("read_name")
            .agg(
                pl.col(f"ref{wildcards.haplotype}_start").min(),
                pl.col(f"ref{wildcards.haplotype}_end").max(),
            )
        ).collect()

        # Write a bed file for processing with bedtools
        (cov_df
            .select([
                pl.lit(wildcards.chrom + "_RagTag").alias("chrom"), 
                pl.col(f"ref{wildcards.haplotype}_start"),
                pl.col(f"ref{wildcards.haplotype}_end"),
                "read_name",
            ])
            .filter(pl.col(f"ref{wildcards.haplotype}_start") < pl.col(f"ref{wildcards.haplotype}_end"))
            .write_csv(
                output.cov_bed,
                separator = "\t",
                include_header = False,
            )
        )

        # Find the intersection to the coverage; set the coverage to 0 artifically when there is no overlap
        shell(
            "{bedtools_path} intersect "
            "-a {output.cov_bed} "
            "-b {input.bedgraph} "
            "-wao -bed | "
            "awk -v OFS='\t' -F'\t' '{{ if ($9 == 0) $8 = 0; print }}' | "
            "{bedtools_path} groupby -i stdin -g 1,2,3,4 -c 8 -o min "
            " > {output.intersect_bed}"
        )
        
        # Write to parquet
        pl.read_csv(
            output.intersect_bed,            
            new_columns = ["chrom", f"ref{wildcards.haplotype}_start", f"ref{wildcards.haplotype}_end", "read_name", coverage_col],
            has_header = False,
            separator = "\t",
        ).write_parquet(output.cov_parquet)

rule coverage_for_all_reads_all:
    input:
        cov_parquet = [str(output_path / f"read_analysis/{R.sample_set}/{R.sample_id}/reads/{chrom}/haplotype_{haplotype}/all_reads.coverage_refinement.parquet") 
                for R in data_rows
                for chrom in aut_chrom_names
                for haplotype in [1,2]]



# ------------------------------------------------------------------------------------------------------------------------
# 5. Generate candidate reads + dashboard
#
def get_all_high_conf_parquets_for_sample_id(wildcards):
    res = []
    for R in data_rows:
        if R.sample_id == wildcards.sample_id:
            res.append(
                output_path / f"read_analysis/{R.sample_set}/{R.sample_id}/{R.flow_cell}/{wildcards.chrom}/annotated_0.95_high_confidence_snps.parquet"
            )
    return res

rule find_candidate_reads:
    input:
        parquets = get_all_high_conf_parquets_for_sample_id,
    output:
        all_parquet = output_path / "read_analysis/{sample_set}/{sample_id}/reads/{chrom}/all_reads.parquet",
        candidates_parquet = output_path / "read_analysis/{sample_set}/{sample_id}/reads/{chrom}/candidate_reads.parquet",
    resources:
        mem_mb = 16000,
    run:
        annotated_snps_df = pl.scan_parquet(input.parquets)

        print("Parquets:", input.parquets)

        # First make stats for all reads, just high-confidence but not high-quality SNPs, like in the phasing coverage
        all_stats_df = diagnostics.snps_to_read_stats(
            annotated_snps_df,
            pl.col("is_high_conf_snp"),
            "frac_fits1_more_snps_high_conf",    
        )

        # Add some read-specific information
        all_stats_df = all_stats_df.join(
            (annotated_snps_df
                .select("read_name", "mapq1", "mapq2", "is_forward1", "is_forward2", "total_mismatches", "num_common_insertions", "num_common_deletions", "total_clipping")
                .unique()
            ),
            on="read_name",
            how="left",
        )
        all_stats_df.sink_parquet(output.all_parquet)

        # Then just for candidates
        hap_stats_df = diagnostics.snps_to_read_stats(
            annotated_snps_df,
            pl.col("is_high_quality_snp"),
            "frac_fits1_super_conf",    
        )
        hap_stats_df = hap_stats_df.filter(~((pl.col("frac_fits1_super_conf") == 0) | (pl.col("frac_fits1_super_conf") == 1)))

        hap_stats_df.sink_parquet(output.candidates_parquet)

rule find_candidate_reads_final:
    input:
        parquet = [str(output_path / f"read_analysis/{R.sample_set}/{R.sample_id}/reads/{chrom}/all_reads.parquet") \
            for R in data_df.drop_duplicates(subset=["sample_set", "sample_id"]).itertuples()
            for chrom in aut_chrom_names]

rule create_dashboard_bams:
    input:
        all_reads_parquet = output_path / "read_analysis/{sample_set}/{sample_id}/reads/{chrom}/all_reads.parquet",
        candidate_reads_parquet = output_path / "read_analysis/{sample_set}/{sample_id}/reads/{chrom}/candidate_reads.parquet",
        denovo_hap1_alignment_bams = lambda wc: [
            str(output_path / f"alignments/{wc.sample_set}/{wc.sample_id}/{R.flow_cell}/T2T_scaffolds/haplotype_1" \
                / "minimap2.sorted.primary_alignments.bam")
                for R in data_rows if R.sample_id == wc.sample_id
        ],        
        denovo_hap2_alignment_bams = lambda wc: [
            str(output_path / f"alignments/{wc.sample_set}/{wc.sample_id}/{R.flow_cell}/T2T_scaffolds/haplotype_2" \
                / "minimap2.sorted.primary_alignments.bam")
                for R in data_rows if R.sample_id == wc.sample_id
        ],
    output:
        plots_dir = directory(output_path / "plots/{sample_set}/{sample_id}/{chrom}/"),
        done = output_path / "plots/{sample_set}/{sample_id}/{chrom}/done",
    run:
        dashboard.write_all_bams(
            input.all_reads_parquet,
            input.candidate_reads_parquet,
            input.denovo_hap1_alignment_bams,
            output.plots_dir,
            "hap1.bam",
            wildcards.chrom + "_RagTag",
            "frac_fits1_more_snps_high_conf",
        )

        dashboard.write_all_bams(
            input.all_reads_parquet,
            input.candidate_reads_parquet,
            input.denovo_hap2_alignment_bams,
            output.plots_dir,
            "hap2.bam",
            wildcards.chrom + "_RagTag",
            "frac_fits1_more_snps_high_conf",
        )

        # Make sure that there is at least one non-empty file if finished correctly
        shell("touch {output.done}")

rule create_dashboard_bams_final:
    input:
        plots_dir = [str(output_path / f"plots/{R.sample_set}/{R.sample_id}/{chrom}/")
            for R in data_rows
            for chrom in aut_chrom_names]
    
# ------------------------------------------------------------------------------------------------------------------------
# 6. Classify candidate reads
#
rule classify_reads:
    input:
        events_parquets = lambda wc: [
            str(output_path / f"read_analysis/{wc.sample_set}/{wc.sample_id}/{R.flow_cell}/{wc.chrom}/annotated_0.95_high_confidence_snps.parquet")
                for R in data_rows if R.sample_id == wc.sample_id
        ],
        candidate_reads_parquet = output_path / "read_analysis/{sample_set}/{sample_id}/reads/{chrom}/candidate_reads.parquet",
        cov1_parquet = output_path / "read_analysis/{sample_set}/{sample_id}/reads/{chrom}/haplotype_1/candidate_reads.coverage_refinement.parquet",
        cov2_parquet = output_path / "read_analysis/{sample_set}/{sample_id}/reads/{chrom}/haplotype_2/candidate_reads.coverage_refinement.parquet",
    output:
        classified_reads_parquet = output_path / "read_analysis/{sample_set}/{sample_id}/reads/{chrom}/classified_reads.parquet",
    resources: 
        mem_mb = 64000,
    run:
        P = config["qc_parameters"]

        # TODO: Parameterize better
        basic_filtering = \
            (~pl.col("has_common_transition")) & \
            (pl.col("min_coverage_between_transitions_hap1") >= P["phased_coverage_min"]) & \
            (pl.col("min_coverage_between_transitions_hap2") >= P["phased_coverage_min"]) & \
            (pl.col("mapq1") >= P["map_qual_min"]) & \
            (pl.col("mapq2") >= P["map_qual_min"]) & \
            (pl.col("is_forward1") == pl.col("is_forward2"))

        extra_filtering = \
            (pl.col("total_mismatches") <= P["total_mismatches_max"]) & \
            (pl.col("total_clipping") <= P["total_clipping_max"]) & \
            (~pl.col("read_name").is_in(IDs.NCO_read_blacklist))
            
        res_df = diagnostics.classify_all_reads(
            pl.read_parquet(input.events_parquets),
            pl.read_parquet(input.candidate_reads_parquet),
            pl.read_parquet(input.cov1_parquet),
            pl.read_parquet(input.cov2_parquet),
            basic_filtering & extra_filtering,           
        )
    
        res_df = res_df.with_columns(
            pl.lit(wildcards.chrom).alias("chrom"), 
            pl.lit(wildcards.sample_id).alias("sample_id"),
            pl.lit(wildcards.sample_set).alias("sample_set"),
        )

        res_df.write_parquet(output.classified_reads_parquet)

rule classify_reads_final:
    input:
        plots_dir = [str(output_path / f"read_analysis/{R.sample_set}/{R.sample_id}/reads/{chrom}/classified_reads.parquet")
            for R in data_rows
            for chrom in aut_chrom_names]

# ------------------------------------------------------------------------------------------------------------------------
# 7. Annotate all reads structure for analysis
#
rule annotate_all_reads_structure:
    input:
        parquet = lambda wc: [
            str(output_path / f"read_analysis/{wc.sample_set}/{wc.sample_id}/{R.flow_cell}/{wc.chrom}/" \
                / "annotated_0.95_high_confidence_snps.parquet")
            for R in data_rows if R.sample_id == wc.sample_id
        ],

        csv_grch37 = lambda wc: [
            str(output_path / f"alignments/{wc.sample_set}/{wc.sample_id}/{R.flow_cell}/grch37_reference/" \
                / "minimap2.sorted.primary_alignments.ref_starts.csv.gz")
            for R in data_rows if R.sample_id == wc.sample_id
        ],
        csv_grch38 = lambda wc: [
            str(output_path / f"alignments/{wc.sample_set}/{wc.sample_id}/{R.flow_cell}/grch38_reference/" \
                / "minimap2.sorted.primary_alignments.ref_starts.csv.gz")
            for R in data_rows if R.sample_id == wc.sample_id
        ],
        csv_T2T = lambda wc: [
            str(output_path / f"alignments/{wc.sample_set}/{wc.sample_id}/{R.flow_cell}/T2T_reference/" \
                / "minimap2.sorted.primary_alignments.ref_starts.csv.gz")
            for R in data_rows if R.sample_id == wc.sample_id
        ],

        cov_hap1_parquet = output_path / "read_analysis/{sample_set}/{sample_id}/reads/{chrom}/haplotype_1/all_reads.coverage_refinement.parquet",
        cov_hap2_parquet = output_path / "read_analysis/{sample_set}/{sample_id}/reads/{chrom}/haplotype_2/all_reads.coverage_refinement.parquet",

        AA_hotspots = "/lustre/scratch122/tol/projects/sperm/data/references/06.hotspots/hinch_2023_AA_hotspots.csv",
        CL4_hotspots = "/lustre/scratch122/tol/projects/sperm/data/references/06.hotspots/hinch_2023_CL4_hotspots.csv",
        H3K4me3 = "/lustre/scratch122/tol/projects/sperm/data/references/08.ENCODE/01.H3K4me3_testis/ENCFF608MKN.bigWig",            
        CTCF = "/lustre/scratch122/tol/projects/sperm/data/references/08.ENCODE/02.CTCF_testis/ENCFF343ULH.bigWig",

        classified_reads_parquet = output_path / "read_analysis/{sample_set}/{sample_id}/reads/{chrom}/classified_reads.parquet"
    output:
        all_parquet = output_path / "read_analysis/{sample_set}/{sample_id}/reads/{chrom}/all_reads_structure_annotated.parquet",
    resources:
        time_min = 100,
        mem_mb = 64000,
    run:
        P = config["qc_parameters"]

        reads_df = annotate.annotate_read_structure(
            input.parquet,
            wildcards.sample_id,
            wildcards.chrom,
            input.csv_grch37,
            input.csv_grch38,
            input.csv_T2T, 
            input.cov_hap1_parquet,
            input.cov_hap2_parquet,
            input.AA_hotspots,
            input.CL4_hotspots,
            input.H3K4me3,
            input.CTCF,
            input.classified_reads_parquet,
            GC_tract_mean = P["GC_tract_mean"],
            min_mapq = P["map_qual_min"],
            max_total_mismatches = P["total_mismatches_max"],
            max_total_clipping = P["total_clipping_max"],
            read_margin_in_bp = P["read_margin_in_bp"],
        )

        reads_df.write_parquet(output.all_parquet)

rule annotate_all_reads_structure_final:
    input:
        all_parquet = [str(
            output_path / f"read_analysis/{R.sample_set}/{R.sample_id}/reads/{chrom}/all_reads_structure_annotated.parquet"
        )
            for R in data_rows
            for chrom in aut_chrom_names]